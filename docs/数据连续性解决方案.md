# ğŸ›¡ï¸ æ•°æ®è¿ç»­æ€§å’Œæ•…éšœæ¢å¤è§£å†³æ–¹æ¡ˆ

## ğŸ¯ æ ¸å¿ƒé—®é¢˜åˆ†æ

### å¼€å‘é˜¶æ®µå¸¸è§é—®é¢˜ï¼š
1. **æœåŠ¡é‡å¯** â†’ æ•°æ®ä¸¢å¤±
2. **ç½‘ç»œä¸­æ–­** â†’ æ•°æ®æ–­å±‚  
3. **ç³»ç»Ÿå´©æºƒ** â†’ çŠ¶æ€ä¸¢å¤±
4. **æ•°æ®ä¸ä¸€è‡´** â†’ Rediså’ŒClickHouseæ•°æ®å·®å¼‚
5. **å†·å¯åŠ¨** â†’ ç¼“å­˜ä¸ºç©ºï¼ŒæŸ¥è¯¢æ…¢

## ğŸ”„ å®Œæ•´è§£å†³æ–¹æ¡ˆ

### 1. **Kafka Offsetç®¡ç† - è§£å†³é‡å¯æ•°æ®ä¸¢å¤±**

```rust
// æ¶ˆè´¹è€…ç»„é…ç½® - ç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±
pub struct KafkaConsumerConfig {
    pub group_id: String,
    pub auto_offset_reset: "earliest",  // ä»æœ€æ—©æ¶ˆæ¯å¼€å§‹
    pub enable_auto_commit: false,      // æ‰‹åŠ¨æäº¤offset
    pub session_timeout_ms: 30000,
    pub max_poll_records: 1000,
}

// æ‰‹åŠ¨æäº¤offsetç¡®ä¿æ•°æ®å¤„ç†å®Œæˆ
impl DataProcessor {
    pub async fn process_with_offset_management(&self) -> Result<()> {
        let mut consumer = self.create_consumer().await?;
        
        loop {
            let messages = consumer.poll(Duration::from_millis(1000)).await?;
            
            for message in messages {
                // 1. å¤„ç†æ¶ˆæ¯
                match self.process_message(&message).await {
                    Ok(_) => {
                        // 2. åªæœ‰å¤„ç†æˆåŠŸæ‰æäº¤offset
                        consumer.commit_message(&message).await?;
                        info!("Message processed and committed: offset {}", message.offset());
                    }
                    Err(e) => {
                        // 3. å¤„ç†å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä¸æäº¤offset
                        error!("Message processing failed: {}, will retry", e);
                        // æ¶ˆæ¯ä¼šåœ¨ä¸‹æ¬¡é‡å¯æ—¶é‡æ–°å¤„ç†
                    }
                }
            }
        }
    }
}
```

### 2. **æ•°æ®æ–­å±‚æ£€æµ‹å’Œè¡¥é½æœºåˆ¶**

```rust
// æ•°æ®è¿ç»­æ€§æ£€æŸ¥å™¨
pub struct DataContinuityChecker {
    last_timestamps: HashMap<String, i64>,  // æ¯ä¸ªäº¤æ˜“å¯¹çš„æœ€åæ—¶é—´æˆ³
    gap_threshold: Duration,                // æ•°æ®æ–­å±‚é˜ˆå€¼
    recovery_api: ExchangeRestAPI,          // REST APIè¡¥é½æ•°æ®
}

impl DataContinuityChecker {
    // æ£€æµ‹æ•°æ®æ–­å±‚
    pub async fn check_data_gap(&mut self, symbol: &str, timestamp: i64) -> Result<()> {
        if let Some(last_ts) = self.last_timestamps.get(symbol) {
            let gap = Duration::from_millis((timestamp - last_ts) as u64);
            
            if gap > self.gap_threshold {
                warn!("Data gap detected for {}: {}ms", symbol, gap.as_millis());
                
                // è‡ªåŠ¨è¡¥é½æ•°æ®
                self.fill_data_gap(symbol, *last_ts, timestamp).await?;
            }
        }
        
        self.last_timestamps.insert(symbol.to_string(), timestamp);
        Ok(())
    }
    
    // è¡¥é½æ•°æ®æ–­å±‚
    async fn fill_data_gap(&self, symbol: &str, start_ts: i64, end_ts: i64) -> Result<()> {
        info!("Filling data gap for {} from {} to {}", symbol, start_ts, end_ts);
        
        // 1. ä»äº¤æ˜“æ‰€REST APIè·å–å†å²æ•°æ®
        let historical_data = self.recovery_api.get_klines(
            symbol,
            "1m",
            start_ts,
            end_ts
        ).await?;
        
        // 2. æ ‡è®°ä¸ºè¡¥é½æ•°æ®å¹¶å­˜å‚¨
        for mut kline in historical_data {
            kline.is_backfilled = true;  // æ ‡è®°ä¸ºè¡¥é½æ•°æ®
            
            // 3. ç›´æ¥å†™å…¥å­˜å‚¨ï¼Œè·³è¿‡Kafkaï¼ˆé¿å…é‡å¤å¤„ç†ï¼‰
            self.storage_manager.store_backfilled_data(&kline).await?;
        }
        
        info!("Data gap filled: {} records for {}", historical_data.len(), symbol);
        Ok(())
    }
}
```

### 3. **æœåŠ¡çŠ¶æ€æŒä¹…åŒ– - è§£å†³é‡å¯çŠ¶æ€ä¸¢å¤±**

```rust
// æœåŠ¡çŠ¶æ€ç®¡ç†å™¨
pub struct ServiceStateManager {
    state_file: PathBuf,
    redis: Arc<RedisStorage>,
}

#[derive(Serialize, Deserialize)]
pub struct ServiceState {
    pub last_processed_timestamps: HashMap<String, i64>,
    pub active_subscriptions: Vec<String>,
    pub consumer_offsets: HashMap<String, i64>,
    pub startup_time: i64,
    pub shutdown_time: Option<i64>,
}

impl ServiceStateManager {
    // ä¿å­˜æœåŠ¡çŠ¶æ€
    pub async fn save_state(&self, state: &ServiceState) -> Result<()> {
        // 1. ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        let state_json = serde_json::to_string_pretty(state)?;
        tokio::fs::write(&self.state_file, state_json).await?;
        
        // 2. å¤‡ä»½åˆ°Redis
        self.redis.set("service_state:market_data", state, 86400).await?;
        
        debug!("Service state saved");
        Ok(())
    }
    
    // æ¢å¤æœåŠ¡çŠ¶æ€
    pub async fn load_state(&self) -> Result<Option<ServiceState>> {
        // 1. ä¼˜å…ˆä»æœ¬åœ°æ–‡ä»¶æ¢å¤
        if let Ok(state_json) = tokio::fs::read_to_string(&self.state_file).await {
            if let Ok(state) = serde_json::from_str::<ServiceState>(&state_json) {
                info!("Service state loaded from file");
                return Ok(Some(state));
            }
        }
        
        // 2. ä»Redisæ¢å¤
        if let Ok(Some(state)) = self.redis.get::<ServiceState>("service_state:market_data").await {
            info!("Service state loaded from Redis");
            return Ok(Some(state));
        }
        
        warn!("No previous service state found, starting fresh");
        Ok(None)
    }
    
    // å®šæœŸä¿å­˜çŠ¶æ€
    pub fn start_periodic_save(&self, state: Arc<RwLock<ServiceState>>) {
        let manager = self.clone();
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_secs(30));
            
            loop {
                interval.tick().await;
                
                let current_state = state.read().await.clone();
                if let Err(e) = manager.save_state(&current_state).await {
                    error!("Failed to save service state: {}", e);
                }
            }
        });
    }
}
```

### 4. **ä¼˜é›…å…³æœºå’Œå¯åŠ¨æœºåˆ¶**

```rust
// ä¼˜é›…å…³æœºå¤„ç†å™¨
pub struct GracefulShutdownHandler {
    shutdown_signal: Arc<tokio::sync::Notify>,
    services: Vec<Box<dyn ShutdownService>>,
}

#[async_trait]
pub trait ShutdownService: Send + Sync {
    async fn shutdown(&mut self) -> Result<()>;
}

impl GracefulShutdownHandler {
    pub async fn wait_for_shutdown(&self) {
        // ç›‘å¬å…³æœºä¿¡å·
        tokio::select! {
            _ = tokio::signal::ctrl_c() => {
                info!("Received Ctrl+C, initiating graceful shutdown");
            }
            _ = self.shutdown_signal.notified() => {
                info!("Received shutdown signal");
            }
        }
        
        // æ‰§è¡Œä¼˜é›…å…³æœº
        self.perform_shutdown().await;
    }
    
    async fn perform_shutdown(&self) {
        info!("Starting graceful shutdown...");
        
        // 1. åœæ­¢æ¥æ”¶æ–°æ•°æ®
        info!("Stopping data ingestion...");
        
        // 2. å¤„ç†å®Œæ‰€æœ‰ç¼“å†²åŒºæ•°æ®
        info!("Flushing all buffers...");
        
        // 3. ä¿å­˜æœåŠ¡çŠ¶æ€
        info!("Saving service state...");
        
        // 4. å…³é—­æ‰€æœ‰æœåŠ¡
        for (i, service) in self.services.iter().enumerate() {
            info!("Shutting down service {}...", i);
            if let Err(e) = service.shutdown().await {
                error!("Failed to shutdown service {}: {}", i, e);
            }
        }
        
        info!("Graceful shutdown completed");
    }
}

// å¸‚åœºæ•°æ®æœåŠ¡çš„å…³æœºå®ç°
#[async_trait]
impl ShutdownService for MarketDataService {
    async fn shutdown(&mut self) -> Result<()> {
        // 1. åœæ­¢WebSocketè¿æ¥
        self.exchange_manager.disconnect_all().await?;
        
        // 2. åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº
        self.data_processor.flush_all_buffers().await?;
        self.storage_manager.flush_all().await?;
        
        // 3. æäº¤Kafka offset
        self.kafka_consumer.commit_sync().await?;
        
        // 4. ä¿å­˜æœ€åçŠ¶æ€
        let state = ServiceState {
            last_processed_timestamps: self.get_last_timestamps(),
            shutdown_time: Some(chrono::Utc::now().timestamp_millis()),
            ..Default::default()
        };
        self.state_manager.save_state(&state).await?;
        
        // 5. å…³é—­å­˜å‚¨è¿æ¥
        self.storage_manager.shutdown().await?;
        
        Ok(())
    }
}
```

### 5. **æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®å¤**

```rust
// æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨
pub struct DataConsistencyChecker {
    redis: Arc<RedisStorage>,
    clickhouse: Arc<ClickHouseStorage>,
}

impl DataConsistencyChecker {
    // æ£€æŸ¥Rediså’ŒClickHouseæ•°æ®ä¸€è‡´æ€§
    pub async fn check_consistency(&self, symbol: &str) -> Result<ConsistencyReport> {
        let mut report = ConsistencyReport::new(symbol);
        
        // 1. è·å–Redisä¸­çš„æœ€æ–°æ•°æ®
        let redis_tick = self.redis.get_latest_tick(symbol).await?;
        
        // 2. è·å–ClickHouseä¸­çš„æœ€æ–°æ•°æ®
        let ch_tick = self.clickhouse.get_latest_tick(symbol).await?;
        
        // 3. æ¯”è¾ƒæ—¶é—´æˆ³
        match (redis_tick, ch_tick) {
            (Some(redis), Some(ch)) => {
                let time_diff = (redis.timestamp - ch.timestamp).abs();
                
                if time_diff > 60000 { // è¶…è¿‡1åˆ†é’Ÿå·®å¼‚
                    report.add_issue(ConsistencyIssue::TimestampMismatch {
                        redis_ts: redis.timestamp,
                        clickhouse_ts: ch.timestamp,
                        diff_ms: time_diff,
                    });
                }
                
                // 4. æ¯”è¾ƒä»·æ ¼
                if (redis.price - ch.price).abs() > Decimal::new(1, 4) { // 0.0001å·®å¼‚
                    report.add_issue(ConsistencyIssue::PriceMismatch {
                        redis_price: redis.price,
                        clickhouse_price: ch.price,
                    });
                }
            }
            (Some(_), None) => {
                report.add_issue(ConsistencyIssue::MissingInClickHouse);
            }
            (None, Some(_)) => {
                report.add_issue(ConsistencyIssue::MissingInRedis);
            }
            (None, None) => {
                report.add_issue(ConsistencyIssue::NoDataFound);
            }
        }
        
        Ok(report)
    }
    
    // ä¿®å¤æ•°æ®ä¸ä¸€è‡´
    pub async fn repair_inconsistency(&self, symbol: &str) -> Result<()> {
        let report = self.check_consistency(symbol).await?;
        
        for issue in report.issues {
            match issue {
                ConsistencyIssue::MissingInRedis => {
                    // ä»ClickHouseåŒæ­¥åˆ°Redis
                    if let Some(tick) = self.clickhouse.get_latest_tick(symbol).await? {
                        self.redis.store_tick(&tick).await?;
                        info!("Synced missing data from ClickHouse to Redis for {}", symbol);
                    }
                }
                ConsistencyIssue::MissingInClickHouse => {
                    // ä»RedisåŒæ­¥åˆ°ClickHouse
                    if let Some(tick) = self.redis.get_latest_tick(symbol).await? {
                        self.clickhouse.store_tick(&tick).await?;
                        info!("Synced missing data from Redis to ClickHouse for {}", symbol);
                    }
                }
                ConsistencyIssue::TimestampMismatch { .. } => {
                    // ä»¥ClickHouseä¸ºå‡†ï¼Œæ›´æ–°Redis
                    if let Some(tick) = self.clickhouse.get_latest_tick(symbol).await? {
                        self.redis.store_tick(&tick).await?;
                        info!("Fixed timestamp mismatch for {}", symbol);
                    }
                }
                _ => {
                    warn!("Cannot auto-repair issue: {:?}", issue);
                }
            }
        }
        
        Ok(())
    }
}
```

### 6. **å¯åŠ¨æ—¶æ•°æ®é¢„çƒ­æœºåˆ¶**

```rust
// æ•°æ®é¢„çƒ­å™¨
pub struct DataPreloader {
    redis: Arc<RedisStorage>,
    clickhouse: Arc<ClickHouseStorage>,
}

impl DataPreloader {
    // å¯åŠ¨æ—¶é¢„çƒ­ç¼“å­˜
    pub async fn preload_cache(&self, symbols: &[String]) -> Result<()> {
        info!("Starting cache preload for {} symbols", symbols.len());
        
        let mut tasks = Vec::new();
        
        for symbol in symbols {
            let symbol = symbol.clone();
            let redis = self.redis.clone();
            let clickhouse = self.clickhouse.clone();
            
            let task = tokio::spawn(async move {
                // 1. é¢„çƒ­æœ€æ–°Tickæ•°æ®
                if let Ok(Some(tick)) = clickhouse.get_latest_tick(&symbol).await {
                    let _ = redis.store_tick(&tick).await;
                }
                
                // 2. é¢„çƒ­æœ€æ–°Kçº¿æ•°æ®
                for interval in &["1m", "5m", "15m", "1h", "4h", "1d"] {
                    if let Ok(Some(kline)) = clickhouse.get_latest_kline(&symbol, interval).await {
                        let _ = redis.store_kline(&kline).await;
                    }
                }
                
                info!("Cache preloaded for {}", symbol);
            });
            
            tasks.push(task);
        }
        
        // ç­‰å¾…æ‰€æœ‰é¢„çƒ­ä»»åŠ¡å®Œæˆ
        futures::future::join_all(tasks).await;
        
        info!("Cache preload completed");
        Ok(())
    }
}
```

## ğŸš€ å®Œæ•´å¯åŠ¨æµç¨‹

```rust
// ä¸»å¯åŠ¨å‡½æ•°
#[tokio::main]
async fn main() -> Result<()> {
    // 1. åˆå§‹åŒ–æ—¥å¿—
    LoggingInitializer::init_dev()?;
    
    // 2. åŠ è½½é…ç½®
    let config = MarketDataConfig::load()?;
    
    // 3. æ¢å¤æœåŠ¡çŠ¶æ€
    let state_manager = ServiceStateManager::new("./data/service_state.json");
    let previous_state = state_manager.load_state().await?;
    
    // 4. åˆå§‹åŒ–å­˜å‚¨
    let storage_manager = Arc::new(StorageManager::new(config.clone()).await?);
    
    // 5. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    let consistency_checker = DataConsistencyChecker::new(
        storage_manager.get_redis(),
        storage_manager.get_clickhouse()
    );
    
    for symbol in &config.all_symbols() {
        if let Err(e) = consistency_checker.repair_inconsistency(symbol).await {
            warn!("Failed to repair consistency for {}: {}", symbol, e);
        }
    }
    
    // 6. é¢„çƒ­ç¼“å­˜
    let preloader = DataPreloader::new(
        storage_manager.get_redis(),
        storage_manager.get_clickhouse()
    );
    preloader.preload_cache(&config.all_symbols()).await?;
    
    // 7. åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨ï¼ˆä»ä¸Šæ¬¡offsetç»§ç»­ï¼‰
    let mut data_processor = DataProcessor::new(config.clone(), storage_manager.clone()).await?;
    if let Some(state) = previous_state {
        data_processor.restore_from_state(&state).await?;
    }
    
    // 8. å¯åŠ¨æ•°æ®è¿ç»­æ€§æ£€æŸ¥
    let continuity_checker = DataContinuityChecker::new(config.clone());
    data_processor.set_continuity_checker(continuity_checker);
    
    // 9. å¯åŠ¨æœåŠ¡
    let exchange_manager = Arc::new(ExchangeManager::new(config.clone()).await?);
    exchange_manager.start_all_connections().await?;
    
    // 10. è®¾ç½®ä¼˜é›…å…³æœº
    let shutdown_handler = GracefulShutdownHandler::new();
    shutdown_handler.add_service(Box::new(data_processor));
    shutdown_handler.add_service(Box::new(exchange_manager));
    
    // 11. å¯åŠ¨å®šæœŸçŠ¶æ€ä¿å­˜
    let current_state = Arc::new(RwLock::new(ServiceState::new()));
    state_manager.start_periodic_save(current_state.clone());
    
    info!("ğŸš€ Market Data Service started successfully");
    
    // 12. ç­‰å¾…å…³æœºä¿¡å·
    shutdown_handler.wait_for_shutdown().await;
    
    Ok(())
}
```

## ğŸ¯ å¼€å‘é˜¶æ®µä½¿ç”¨æŒ‡å—

### æ—¥å¸¸å¼€å‘æµç¨‹ï¼š
```bash
# 1. å¯åŠ¨æœåŠ¡ï¼ˆè‡ªåŠ¨æ¢å¤çŠ¶æ€ï¼‰
cargo run

# 2. å¼€å‘è°ƒè¯•ï¼ˆæœåŠ¡ç»§ç»­è¿è¡Œï¼‰
# ä¿®æ”¹ä»£ç ...

# 3. é‡å¯æœåŠ¡ï¼ˆCtrl+Cä¼˜é›…å…³æœºï¼Œè‡ªåŠ¨ä¿å­˜çŠ¶æ€ï¼‰
# é‡æ–°å¯åŠ¨ä¼šä»ä¸Šæ¬¡çŠ¶æ€ç»§ç»­

# 4. æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
curl http://localhost:8081/api/v1/admin/consistency-check

# 5. æ‰‹åŠ¨ä¿®å¤æ•°æ®
curl -X POST http://localhost:8081/api/v1/admin/repair-data
```

è¿™æ ·çš„è®¾è®¡ç¡®ä¿äº†ï¼š
- âœ… **é›¶æ•°æ®ä¸¢å¤±**ï¼šKafka offsetç®¡ç† + çŠ¶æ€æŒä¹…åŒ–
- âœ… **è‡ªåŠ¨æ¢å¤**ï¼šæ–­å±‚æ£€æµ‹ + REST APIè¡¥é½
- âœ… **æ•°æ®ä¸€è‡´æ€§**ï¼šå®šæœŸæ£€æŸ¥ + è‡ªåŠ¨ä¿®å¤
- âœ… **å¿«é€Ÿå¯åŠ¨**ï¼šç¼“å­˜é¢„çƒ­ + çŠ¶æ€æ¢å¤
- âœ… **å¼€å‘å‹å¥½**ï¼šä¼˜é›…å…³æœº + çŠ¶æ€ä¿å­˜