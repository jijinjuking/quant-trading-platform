# è¿ç»´éƒ¨ç½²æŒ‡å¯¼æ–‡æ¡£
## DevOps Deployment Guide v1.0

---

## ğŸ¯ è¿ç»´å·¥ç¨‹å¸ˆèŒè´£

ä½ è´Ÿè´£**åŸºç¡€è®¾æ–½æ­å»ºã€éƒ¨ç½²è‡ªåŠ¨åŒ–ã€ç›‘æ§å‘Šè­¦ã€æ€§èƒ½ä¼˜åŒ–**ç­‰è¿ç»´å·¥ä½œã€‚

**æ ¸å¿ƒç›®æ ‡**:
- ğŸš€ ç¡®ä¿ç³»ç»Ÿé«˜å¯ç”¨æ€§ (99.9%+)
- âš¡ ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ (å»¶è¿Ÿ<100ms)
- ğŸ”’ ä¿éšœç³»ç»Ÿå®‰å…¨æ€§
- ğŸ“Š å»ºç«‹å®Œå–„ç›‘æ§ä½“ç³»

---

## ğŸ—ï¸ åŸºç¡€è®¾æ–½æ¶æ„

### æœåŠ¡ç«¯å£åˆ†é…
```
API Gateway:        8080
Market Data:        8081  
Trading Engine:     8082
Strategy Engine:    8083
AI Service:         8084
Risk Management:    8085
User Management:    8086
Notification:       8087
Analytics:          8088

PostgreSQL:         5432
Redis:              6379
ClickHouse:         9000 (native), 8123 (http)
Kafka:              9092
Zookeeper:          2181
```

### ç½‘ç»œæ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Load Balancer (Nginx)                    â”‚
â”‚                         Port 80/443                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    API Gateway Cluster                      â”‚
â”‚                    (3 instances: 8080)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Market Data â”‚ Trading Engine â”‚ Strategy Engine â”‚ AI Service â”‚
â”‚  (2 instances) â”‚ (3 instances)  â”‚ (2 instances)   â”‚(1 instance)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Risk Mgmt    â”‚ User Mgmt      â”‚ Notification    â”‚ Analytics  â”‚
â”‚(2 instances) â”‚ (2 instances)  â”‚ (2 instances)   â”‚(1 instance)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PostgreSQL   â”‚ Redis Cluster  â”‚ ClickHouse      â”‚ Kafka      â”‚
â”‚ (Master+Slave)â”‚ (3 nodes)     â”‚ (3 nodes)       â”‚(3 brokers) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Phase 1 è¿ç»´ä»»åŠ¡ (Week 1-2)

### ä»»åŠ¡1: Dockerå¼€å‘ç¯å¢ƒæ­å»º ğŸ³ **P0ä¼˜å…ˆçº§**

**ç›®æ ‡**: ä¸ºå¼€å‘å›¢é˜Ÿæä¾›ç»Ÿä¸€çš„å¼€å‘ç¯å¢ƒ

**å…·ä½“ä»»åŠ¡**:
1. åˆ›å»ºå®Œæ•´çš„ `docker-compose.yml`
2. é…ç½®æ‰€æœ‰åŸºç¡€è®¾æ–½æœåŠ¡
3. è®¾ç½®æ•°æ®æŒä¹…åŒ–
4. é…ç½®ç½‘ç»œå’Œç«¯å£æ˜ å°„

**Docker Compose é…ç½®**:
```yaml
# docker-compose.dev.yml
version: '3.8'

services:
  # åŸºç¡€è®¾æ–½æœåŠ¡
  postgres:
    image: postgres:15-alpine
    container_name: trading-postgres
    environment:
      POSTGRES_DB: trading_platform
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-dev_password}
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8 --lc-collate=C --lc-ctype=C"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d
    networks:
      - trading-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin -d trading_platform"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: trading-redis
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD:-dev_password}
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - trading-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: trading-clickhouse
    environment:
      CLICKHOUSE_DB: market_data
      CLICKHOUSE_USER: admin
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-dev_password}
    ports:
      - "9000:9000"
      - "8123:8123"
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./config/clickhouse:/etc/clickhouse-server/config.d
    networks:
      - trading-network
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: trading-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
    networks:
      - trading-network

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: trading-kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true
    ports:
      - "9092:9092"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - trading-network
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 3

  # å¾®æœåŠ¡ (å¼€å‘é˜¶æ®µå…ˆæ³¨é‡Šï¼Œç­‰ä»£ç å®Œæˆåå¯ç”¨)
  # api-gateway:
  #   build: 
  #     context: ./services/api-gateway
  #     dockerfile: Dockerfile.dev
  #   container_name: trading-api-gateway
  #   ports:
  #     - "8080:8080"
  #   environment:
  #     - RUST_LOG=debug
  #     - DATABASE_URL=postgresql://admin:${POSTGRES_PASSWORD:-dev_password}@postgres:5432/trading_platform
  #     - REDIS_URL=redis://:${REDIS_PASSWORD:-dev_password}@redis:6379
  #   depends_on:
  #     - postgres
  #     - redis
  #   networks:
  #     - trading-network

volumes:
  postgres_data:
  redis_data:
  clickhouse_data:
  zookeeper_data:
  kafka_data:

networks:
  trading-network:
    driver: bridge
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰åŸºç¡€è®¾æ–½æœåŠ¡æ­£å¸¸å¯åŠ¨
- [ ] å¥åº·æ£€æŸ¥é€šè¿‡
- [ ] æ•°æ®æŒä¹…åŒ–æ­£å¸¸
- [ ] ç½‘ç»œè¿é€šæ€§æ­£å¸¸

### ä»»åŠ¡2: æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ ğŸ’¾ **P0ä¼˜å…ˆçº§**

**ç›®æ ‡**: è‡ªåŠ¨åŒ–æ•°æ®åº“ç»“æ„åˆå§‹åŒ–

**å…·ä½“ä»»åŠ¡**:
1. åˆ›å»ºPostgreSQLåˆå§‹åŒ–è„šæœ¬
2. åˆ›å»ºClickHouseè¡¨ç»“æ„
3. é…ç½®RedisåŸºç¡€è®¾ç½®
4. åˆ›å»ºKafka Topics

**PostgreSQL åˆå§‹åŒ–è„šæœ¬**:
```sql
-- migrations/001_init_database.sql
-- ç”¨æˆ·è¡¨
CREATE TABLE IF NOT EXISTS users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    membership_tier VARCHAR(20) DEFAULT 'free' CHECK (membership_tier IN ('free', 'premium', 'pro', 'enterprise')),
    status VARCHAR(20) DEFAULT 'active' CHECK (status IN ('active', 'inactive', 'suspended')),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- è®¢å•è¡¨
CREATE TABLE IF NOT EXISTS orders (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    exchange VARCHAR(20) NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(10) NOT NULL CHECK (side IN ('buy', 'sell')),
    order_type VARCHAR(20) NOT NULL CHECK (order_type IN ('market', 'limit', 'stop', 'stop_limit')),
    quantity DECIMAL(20,8) NOT NULL CHECK (quantity > 0),
    price DECIMAL(20,8) CHECK (price > 0),
    status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'submitted', 'partial_filled', 'filled', 'cancelled', 'rejected', 'failed')),
    filled_quantity DECIMAL(20,8) DEFAULT 0 CHECK (filled_quantity >= 0),
    avg_fill_price DECIMAL(20,8),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- ç­–ç•¥è¡¨
CREATE TABLE IF NOT EXISTS strategies (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    name VARCHAR(100) NOT NULL,
    strategy_type VARCHAR(50) NOT NULL,
    parameters JSONB NOT NULL DEFAULT '{}',
    is_active BOOLEAN DEFAULT false,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- APIå¯†é’¥è¡¨
CREATE TABLE IF NOT EXISTS api_keys (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    exchange VARCHAR(20) NOT NULL,
    key_name VARCHAR(100) NOT NULL,
    api_key_encrypted TEXT NOT NULL,
    api_secret_encrypted TEXT NOT NULL,
    passphrase_encrypted TEXT,
    permissions JSONB DEFAULT '[]',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_orders_user_id ON orders(user_id);
CREATE INDEX IF NOT EXISTS idx_orders_symbol ON orders(symbol);
CREATE INDEX IF NOT EXISTS idx_orders_status ON orders(status);
CREATE INDEX IF NOT EXISTS idx_orders_created_at ON orders(created_at);
CREATE INDEX IF NOT EXISTS idx_strategies_user_id ON strategies(user_id);
CREATE INDEX IF NOT EXISTS idx_api_keys_user_id ON api_keys(user_id);

-- åˆ›å»ºæ›´æ–°æ—¶é—´è§¦å‘å™¨
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_users_updated_at BEFORE UPDATE ON users FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_orders_updated_at BEFORE UPDATE ON orders FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_strategies_updated_at BEFORE UPDATE ON strategies FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
CREATE TRIGGER update_api_keys_updated_at BEFORE UPDATE ON api_keys FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();
```

**ClickHouse åˆå§‹åŒ–è„šæœ¬**:
```sql
-- config/clickhouse/init.sql
-- Tickæ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS market_data.ticks (
    exchange String,
    symbol String,
    timestamp DateTime64(6),
    price Decimal64(8),
    volume Decimal64(8),
    side Enum8('buy' = 1, 'sell' = 2),
    trade_id UInt64,
    is_maker Bool
) ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(timestamp)
ORDER BY (exchange, symbol, timestamp)
SETTINGS index_granularity = 8192;

-- Kçº¿æ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS market_data.klines (
    exchange String,
    symbol String,
    interval String,
    open_time DateTime64(3),
    close_time DateTime64(3),
    open Decimal64(8),
    high Decimal64(8),
    low Decimal64(8),
    close Decimal64(8),
    volume Decimal64(8),
    quote_volume Decimal64(8),
    trades_count UInt32
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(open_time)
ORDER BY (exchange, symbol, interval, open_time)
SETTINGS index_granularity = 8192;

-- ç­–ç•¥æ‰§è¡Œè®°å½•è¡¨
CREATE TABLE IF NOT EXISTS market_data.strategy_executions (
    strategy_id String,
    user_id String,
    timestamp DateTime64(3),
    signal_type Enum8('buy' = 1, 'sell' = 2, 'hold' = 3),
    confidence Float64,
    market_data String,
    ai_reasoning String,
    execution_latency_ms UInt32
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (strategy_id, timestamp)
SETTINGS index_granularity = 8192;

-- æ€§èƒ½æŒ‡æ ‡è¡¨
CREATE TABLE IF NOT EXISTS market_data.performance_metrics (
    timestamp DateTime64(3),
    metric_name String,
    metric_value Float64,
    tags Map(String, String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMMDD(timestamp)
ORDER BY (metric_name, timestamp)
SETTINGS index_granularity = 8192;
```

**Kafka Topics åˆ›å»ºè„šæœ¬**:
```bash
#!/bin/bash
# scripts/create-kafka-topics.sh

# ç­‰å¾…Kafkaå¯åŠ¨
echo "ç­‰å¾…Kafkaå¯åŠ¨..."
sleep 30

# åˆ›å»ºTopics
docker exec trading-kafka kafka-topics --create --bootstrap-server localhost:9092 --topic market-data --partitions 6 --replication-factor 1
docker exec trading-kafka kafka-topics --create --bootstrap-server localhost:9092 --topic trading-signals --partitions 3 --replication-factor 1
docker exec trading-kafka kafka-topics --create --bootstrap-server localhost:9092 --topic order-events --partitions 3 --replication-factor 1
docker exec trading-kafka kafka-topics --create --bootstrap-server localhost:9092 --topic risk-alerts --partitions 2 --replication-factor 1
docker exec trading-kafka kafka-topics --create --bootstrap-server localhost:9092 --topic user-events --partitions 2 --replication-factor 1

echo "Kafka Topicsåˆ›å»ºå®Œæˆ"
```

### ä»»åŠ¡3: åŸºç¡€ç›‘æ§é…ç½® ğŸ“Š **P1ä¼˜å…ˆçº§**

**ç›®æ ‡**: å»ºç«‹åŸºç¡€çš„ç³»ç»Ÿç›‘æ§

**å…·ä½“ä»»åŠ¡**:
1. é…ç½®Prometheusç›‘æ§
2. é…ç½®Grafanaä»ªè¡¨æ¿
3. è®¾ç½®åŸºç¡€å‘Šè­¦è§„åˆ™
4. é…ç½®æ—¥å¿—æ”¶é›†

**Prometheus é…ç½®**:
```yaml
# config/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'api-gateway'
    static_configs:
      - targets: ['api-gateway:8080']
    metrics_path: '/metrics'

  - job_name: 'market-data'
    static_configs:
      - targets: ['market-data:8081']
    metrics_path: '/metrics'

  - job_name: 'trading-engine'
    static_configs:
      - targets: ['trading-engine:8082']
    metrics_path: '/metrics'

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

**å‘Šè­¦è§„åˆ™é…ç½®**:
```yaml
# config/prometheus/alert_rules.yml
groups:
  - name: trading_platform_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: HighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }} seconds"

      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database is not responding"

      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis cache is not responding"
```

### ä»»åŠ¡4: CI/CDæµæ°´çº¿ ğŸš€ **P1ä¼˜å…ˆçº§**

**ç›®æ ‡**: è‡ªåŠ¨åŒ–æ„å»ºå’Œéƒ¨ç½²æµç¨‹

**GitHub Actions é…ç½®**:
```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  CARGO_TERM_COLOR: always

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: trading_platform_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 3s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v3
    
    - name: Install Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
        components: rustfmt, clippy

    - name: Cache cargo registry
      uses: actions/cache@v3
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}

    - name: Cache cargo index
      uses: actions/cache@v3
      with:
        path: ~/.cargo/git
        key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}

    - name: Cache cargo build
      uses: actions/cache@v3
      with:
        path: target
        key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}

    - name: Check formatting
      run: cargo fmt --all -- --check

    - name: Run clippy
      run: cargo clippy --all-targets --all-features -- -D warnings

    - name: Run tests
      run: cargo test --all-features
      env:
        DATABASE_URL: postgresql://postgres:test_password@localhost:5432/trading_platform_test
        REDIS_URL: redis://localhost:6379

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Login to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Build and push API Gateway
      uses: docker/build-push-action@v4
      with:
        context: ./services/api-gateway
        push: true
        tags: trading-platform/api-gateway:latest

    - name: Build and push Market Data Service
      uses: docker/build-push-action@v4
      with:
        context: ./services/market-data
        push: true
        tags: trading-platform/market-data:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment"
        # è¿™é‡Œæ·»åŠ éƒ¨ç½²è„šæœ¬
```

---

## ğŸ“‹ Phase 2 è¿ç»´ä»»åŠ¡ (Week 3-6)

### ä»»åŠ¡5: ClickHouseé›†ç¾¤éƒ¨ç½² ğŸ“Š **P0ä¼˜å…ˆçº§**

**ç›®æ ‡**: éƒ¨ç½²é«˜å¯ç”¨ClickHouseé›†ç¾¤

**é›†ç¾¤é…ç½®**:
```yaml
# docker-compose.clickhouse-cluster.yml
version: '3.8'

services:
  clickhouse-01:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse-01
    hostname: clickhouse-01
    ports:
      - "9001:9000"
      - "8124:8123"
    volumes:
      - ./config/clickhouse-cluster/clickhouse-01:/etc/clickhouse-server
      - clickhouse_01_data:/var/lib/clickhouse
    networks:
      - clickhouse-cluster

  clickhouse-02:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse-02
    hostname: clickhouse-02
    ports:
      - "9002:9000"
      - "8125:8123"
    volumes:
      - ./config/clickhouse-cluster/clickhouse-02:/etc/clickhouse-server
      - clickhouse_02_data:/var/lib/clickhouse
    networks:
      - clickhouse-cluster

  clickhouse-03:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse-03
    hostname: clickhouse-03
    ports:
      - "9003:9000"
      - "8126:8123"
    volumes:
      - ./config/clickhouse-cluster/clickhouse-03:/etc/clickhouse-server
      - clickhouse_03_data:/var/lib/clickhouse
    networks:
      - clickhouse-cluster

volumes:
  clickhouse_01_data:
  clickhouse_02_data:
  clickhouse_03_data:

networks:
  clickhouse-cluster:
    driver: bridge
```

### ä»»åŠ¡6: Kafkaæ¶ˆæ¯é˜Ÿåˆ—é…ç½® ğŸ“¨ **P0ä¼˜å…ˆçº§**

**ç›®æ ‡**: é…ç½®é«˜å¯ç”¨Kafkaé›†ç¾¤

**Kafkaé›†ç¾¤é…ç½®**:
```yaml
# docker-compose.kafka-cluster.yml
version: '3.8'

services:
  zookeeper-1:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888

  kafka-1:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper-1
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2

  kafka-2:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper-1
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3

  kafka-3:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper-1
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
```

### ä»»åŠ¡7: æ€§èƒ½ç›‘æ§ä¼˜åŒ– âš¡ **P1ä¼˜å…ˆçº§**

**ç›®æ ‡**: å»ºç«‹å®Œå–„çš„æ€§èƒ½ç›‘æ§ä½“ç³»

**Grafanaä»ªè¡¨æ¿é…ç½®**:
```json
{
  "dashboard": {
    "title": "Trading Platform Monitoring",
    "panels": [
      {
        "title": "API Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "Order Processing Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(orders_processed_total[5m])",
            "legendFormat": "Orders/sec"
          }
        ]
      },
      {
        "title": "Market Data Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "market_data_latency_ms",
            "legendFormat": "Latency (ms)"
          }
        ]
      }
    ]
  }
}
```

### ä»»åŠ¡8: å®‰å…¨åŠ å›º ğŸ”’ **P1ä¼˜å…ˆçº§**

**ç›®æ ‡**: ç¡®ä¿ç³»ç»Ÿå®‰å…¨æ€§

**å®‰å…¨é…ç½®æ¸…å•**:
```bash
# 1. é˜²ç«å¢™é…ç½®
sudo ufw enable
sudo ufw allow 22/tcp    # SSH
sudo ufw allow 80/tcp    # HTTP
sudo ufw allow 443/tcp   # HTTPS
sudo ufw deny 5432/tcp   # PostgreSQL (ä»…å†…ç½‘)
sudo ufw deny 6379/tcp   # Redis (ä»…å†…ç½‘)

# 2. SSLè¯ä¹¦é…ç½®
# ä½¿ç”¨Let's Encryptè‡ªåŠ¨åŒ–è¯ä¹¦ç®¡ç†

# 3. æ•°æ®åº“å®‰å…¨
# - ä¿®æ”¹é»˜è®¤å¯†ç 
# - å¯ç”¨SSLè¿æ¥
# - é…ç½®è®¿é—®ç™½åå•

# 4. åº”ç”¨å®‰å…¨
# - JWTå¯†é’¥è½®æ¢
# - APIé™æµé…ç½®
# - è¾“å…¥éªŒè¯
```

---

## ğŸ“‹ Phase 3 è¿ç»´ä»»åŠ¡ (Week 7-12)

### ä»»åŠ¡9: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² ğŸš€ **P0ä¼˜å…ˆçº§**

**Kuberneteséƒ¨ç½²é…ç½®**:
```yaml
# k8s/api-gateway-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  labels:
    app: api-gateway
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
    spec:
      containers:
      - name: api-gateway
        image: trading-platform/api-gateway:latest
        ports:
        - containerPort: 8080
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-secret
              key: url
        - name: REDIS_URL
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: url
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

### ä»»åŠ¡10: è´Ÿè½½å‡è¡¡é…ç½® âš–ï¸ **P1ä¼˜å…ˆçº§**

**Nginxé…ç½®**:
```nginx
# config/nginx/nginx.conf
upstream api_gateway {
    least_conn;
    server api-gateway-1:8080 max_fails=3 fail_timeout=30s;
    server api-gateway-2:8080 max_fails=3 fail_timeout=30s;
    server api-gateway-3:8080 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name trading-platform.com;
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name trading-platform.com;

    ssl_certificate /etc/ssl/certs/trading-platform.crt;
    ssl_certificate_key /etc/ssl/private/trading-platform.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api:10m rate=100r/s;
    limit_req zone=api burst=200 nodelay;

    # APIä»£ç†
    location /api/ {
        proxy_pass http://api_gateway;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 5s;
        proxy_send_timeout 10s;
        proxy_read_timeout 10s;
    }

    # WebSocketä»£ç†
    location /ws/ {
        proxy_pass http://api_gateway;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### ä»»åŠ¡11: å¤‡ä»½æ¢å¤æ–¹æ¡ˆ ğŸ’¾ **P1ä¼˜å…ˆçº§**

**è‡ªåŠ¨åŒ–å¤‡ä»½è„šæœ¬**:
```bash
#!/bin/bash
# scripts/backup.sh

# é…ç½®
BACKUP_DIR="/backup/$(date +%Y%m%d)"
RETENTION_DAYS=30

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# PostgreSQLå¤‡ä»½
echo "å¼€å§‹PostgreSQLå¤‡ä»½..."
docker exec trading-postgres pg_dump -U admin trading_platform | gzip > $BACKUP_DIR/postgres_$(date +%H%M%S).sql.gz

# Rediså¤‡ä»½
echo "å¼€å§‹Rediså¤‡ä»½..."
docker exec trading-redis redis-cli --rdb /data/dump.rdb
docker cp trading-redis:/data/dump.rdb $BACKUP_DIR/redis_$(date +%H%M%S).rdb

# ClickHouseå¤‡ä»½
echo "å¼€å§‹ClickHouseå¤‡ä»½..."
docker exec trading-clickhouse clickhouse-client --query "BACKUP DATABASE market_data TO Disk('backups', 'backup_$(date +%Y%m%d_%H%M%S)')"

# æ¸…ç†æ—§å¤‡ä»½
find /backup -type d -mtime +$RETENTION_DAYS -exec rm -rf {} \;

echo "å¤‡ä»½å®Œæˆ: $BACKUP_DIR"
```

### ä»»åŠ¡12: è¿ç»´è‡ªåŠ¨åŒ– ğŸ¤– **P2ä¼˜å…ˆçº§**

**ç›‘æ§å‘Šè­¦è‡ªåŠ¨åŒ–**:
```yaml
# config/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@trading-platform.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
- name: 'web.hook'
  email_configs:
  - to: 'devops@trading-platform.com'
    subject: '[ALERT] {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      {{ end }}
  
  webhook_configs:
  - url: 'http://slack-webhook:9093/webhook'
    send_resolved: true
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡ç›‘æ§

### å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)
```
ç³»ç»Ÿæ€§èƒ½:
- CPUä½¿ç”¨ç‡: < 70%
- å†…å­˜ä½¿ç”¨ç‡: < 80%
- ç£ç›˜ä½¿ç”¨ç‡: < 85%
- ç½‘ç»œå»¶è¿Ÿ: < 10ms

åº”ç”¨æ€§èƒ½:
- APIå“åº”æ—¶é—´: < 100ms
- è®¢å•å¤„ç†å»¶è¿Ÿ: < 50ms
- å¸‚åœºæ•°æ®å»¶è¿Ÿ: < 10ms
- WebSocketå»¶è¿Ÿ: < 20ms

ä¸šåŠ¡æŒ‡æ ‡:
- ç³»ç»Ÿå¯ç”¨æ€§: > 99.9%
- è®¢å•æˆåŠŸç‡: > 99.5%
- æ•°æ®å‡†ç¡®æ€§: > 99.99%
- ç”¨æˆ·å¹¶å‘æ•°: > 1000
```

### å‘Šè­¦é˜ˆå€¼è®¾ç½®
```
Critical (ç«‹å³å¤„ç†):
- æœåŠ¡å®•æœº
- æ•°æ®åº“è¿æ¥å¤±è´¥
- ç£ç›˜ç©ºé—´ > 90%
- å†…å­˜ä½¿ç”¨ > 90%

Warning (éœ€è¦å…³æ³¨):
- APIå»¶è¿Ÿ > 200ms
- é”™è¯¯ç‡ > 1%
- CPUä½¿ç”¨ > 80%
- è¿æ¥æ•° > é˜ˆå€¼

Info (ä¿¡æ¯è®°å½•):
- éƒ¨ç½²å®Œæˆ
- é…ç½®å˜æ›´
- å®šæœŸå¥åº·æ£€æŸ¥
```

---

## ğŸ”§ è¿ç»´å·¥å…·å’Œè„šæœ¬

### å¸¸ç”¨è¿ç»´å‘½ä»¤
```bash
# æœåŠ¡çŠ¶æ€æ£€æŸ¥
./scripts/health-check.sh

# æ—¥å¿—æŸ¥çœ‹
docker-compose logs -f api-gateway
kubectl logs -f deployment/api-gateway

# æ€§èƒ½ç›‘æ§
docker stats
kubectl top pods

# æ•°æ®åº“è¿æ¥æµ‹è¯•
./scripts/db-test.sh

# æœåŠ¡é‡å¯
docker-compose restart api-gateway
kubectl rollout restart deployment/api-gateway

# æ‰©å®¹ç¼©å®¹
kubectl scale deployment api-gateway --replicas=5
```

### æ•…éšœæ’æŸ¥æ‰‹å†Œ
```
1. æœåŠ¡æ— å“åº”:
   - æ£€æŸ¥å®¹å™¨çŠ¶æ€
   - æŸ¥çœ‹åº”ç”¨æ—¥å¿—
   - æ£€æŸ¥èµ„æºä½¿ç”¨
   - éªŒè¯ç½‘ç»œè¿é€šæ€§

2. æ•°æ®åº“è¿æ¥é—®é¢˜:
   - æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
   - éªŒè¯è¿æ¥å­—ç¬¦ä¸²
   - æ£€æŸ¥ç½‘ç»œç­–ç•¥
   - æŸ¥çœ‹è¿æ¥æ± çŠ¶æ€

3. æ€§èƒ½é—®é¢˜:
   - æŸ¥çœ‹ç›‘æ§æŒ‡æ ‡
   - åˆ†ææ…¢æŸ¥è¯¢
   - æ£€æŸ¥èµ„æºç“¶é¢ˆ
   - ä¼˜åŒ–é…ç½®å‚æ•°

4. å®‰å…¨é—®é¢˜:
   - æ£€æŸ¥è®¿é—®æ—¥å¿—
   - éªŒè¯è¯ä¹¦çŠ¶æ€
   - å®¡æŸ¥æƒé™é…ç½®
   - æ›´æ–°å®‰å…¨è¡¥ä¸
```

---

## ğŸ“ è¿ç»´åä½œæµç¨‹

### æ—¥å¸¸è¿ç»´
- **ç›‘æ§æ£€æŸ¥**: æ¯æ—¥æŸ¥çœ‹ç›‘æ§é¢æ¿
- **æ—¥å¿—åˆ†æ**: å®šæœŸåˆ†æé”™è¯¯æ—¥å¿—
- **æ€§èƒ½ä¼˜åŒ–**: æŒç»­ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½
- **å®‰å…¨æ›´æ–°**: åŠæ—¶åº”ç”¨å®‰å…¨è¡¥ä¸

### åº”æ€¥å“åº”
- **æ•…éšœå‘ç°**: ç›‘æ§å‘Šè­¦ + ç”¨æˆ·åé¦ˆ
- **é—®é¢˜å®šä½**: å¿«é€Ÿå®šä½æ•…éšœåŸå› 
- **åº”æ€¥å¤„ç†**: æ‰§è¡Œåº”æ€¥é¢„æ¡ˆ
- **æ¢å¤éªŒè¯**: ç¡®è®¤æœåŠ¡æ¢å¤æ­£å¸¸

### å˜æ›´ç®¡ç†
- **å˜æ›´ç”³è¯·**: æäº¤å˜æ›´è¯·æ±‚
- **é£é™©è¯„ä¼°**: è¯„ä¼°å˜æ›´é£é™©
- **æµ‹è¯•éªŒè¯**: åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯
- **ç”Ÿäº§å‘å¸ƒ**: æŒ‰è®¡åˆ’å‘å¸ƒåˆ°ç”Ÿäº§

---

**è¿ç»´è´Ÿè´£äºº**: Senior DevOps Engineer  
**æŠ€æœ¯æŒ‡å¯¼**: Kiro AI Architect  
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2024-12-19